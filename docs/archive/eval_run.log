/home/donbr/don-aie-cohort8/cert-challenge/src/config.py:79: UserWarning: Api key is used with an insecure connection.
  return QdrantClient(url=QDRANT_URL, **kwargs)
================================================================================
RAGAS EVALUATION HARNESS (Using src/ Modules)
================================================================================

Start time: 2025-10-20 13:48:20
âœ“ OpenAI API key configured
âœ“ Cohere API key configured (cohere_rerank will work)
âœ“ Linked to ingestion manifest: ragas_pi...

================================================================================
STEP 1: LOADING DATA
================================================================================

Loading source documents from dwb2023/gdelt-rag-sources...
âœ“ Loaded 38 source documents

Loading golden testset from dwb2023/gdelt-rag-golden-testset...
âœ“ Loaded 12 test examples
  Columns: ['user_input', 'reference_contexts', 'reference', 'synthesizer_name']

================================================================================
STEP 2: BUILDING RAG STACK
================================================================================

Creating vector store (recreate=False)...
âœ“ Vector store connected (reusing existing collection)

Creating retrievers (k=5)...
âœ“ Created 4 retrievers: ['naive', 'bm25', 'ensemble', 'cohere_rerank']

Building LangGraph workflows...
âœ“ Built 4 compiled graphs

================================================================================
STEP 3: RUNNING INFERENCE
================================================================================

Processing 12 questions Ã— 4 retrievers...

ğŸ“Š Processing naive retriever...
   âœ“ Processed 12 questions
   ğŸ’¾ Saved inference results: naive_evaluation_inputs.parquet

ğŸ“Š Processing bm25 retriever...
   âœ“ Processed 12 questions
   ğŸ’¾ Saved inference results: bm25_evaluation_inputs.parquet

ğŸ“Š Processing ensemble retriever...
   âœ“ Processed 12 questions
   ğŸ’¾ Saved inference results: ensemble_evaluation_inputs.parquet

ğŸ“Š Processing cohere_rerank retriever...
   âœ“ Processed 12 questions
   ğŸ’¾ Saved inference results: cohere_rerank_evaluation_inputs.parquet

âœ“ All inference complete! Results saved to /home/donbr/don-aie-cohort8/cert-challenge/data/processed

================================================================================
STEP 4: RAGAS EVALUATION
================================================================================

Metrics: Faithfulness, Answer Relevancy, Context Precision, Context Recall

ğŸ” Evaluating naive...
Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]Evaluating:   2%|â–         | 1/48 [00:02<01:34,  2.01s/it]Evaluating:   4%|â–         | 2/48 [00:03<01:12,  1.58s/it]Evaluating:  12%|â–ˆâ–        | 6/48 [00:05<00:29,  1.41it/s]Evaluating:  19%|â–ˆâ–‰        | 9/48 [00:06<00:24,  1.60it/s]Evaluating:  21%|â–ˆâ–ˆ        | 10/48 [00:07<00:24,  1.56it/s]Evaluating:  23%|â–ˆâ–ˆâ–       | 11/48 [00:07<00:22,  1.64it/s]Evaluating:  25%|â–ˆâ–ˆâ–Œ       | 12/48 [00:09<00:26,  1.34it/s]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 13/48 [00:09<00:23,  1.50it/s]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 14/48 [00:10<00:21,  1.61it/s]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–      | 16/48 [00:10<00:18,  1.77it/s]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 18/48 [00:12<00:19,  1.51it/s]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 19/48 [00:12<00:16,  1.79it/s]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 20/48 [00:13<00:15,  1.84it/s]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/48 [00:14<00:21,  1.25it/s]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 22/48 [00:15<00:18,  1.44it/s]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 23/48 [00:18<00:35,  1.42s/it]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/48 [00:18<00:15,  1.44it/s]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 27/48 [00:20<00:17,  1.19it/s]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 28/48 [00:21<00:20,  1.04s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 29/48 [00:22<00:16,  1.14it/s]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 30/48 [00:26<00:29,  1.64s/it]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/48 [00:26<00:21,  1.24s/it]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 32/48 [00:27<00:20,  1.30s/it]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 35/48 [00:29<00:10,  1.22it/s]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 36/48 [00:30<00:11,  1.05it/s]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 37/48 [00:34<00:18,  1.67s/it]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 38/48 [00:36<00:16,  1.60s/it]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 39/48 [00:37<00:13,  1.48s/it]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 40/48 [00:47<00:30,  3.87s/it]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 41/48 [00:49<00:23,  3.32s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 42/48 [01:01<00:34,  5.69s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 43/48 [01:03<00:23,  4.76s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 44/48 [01:05<00:16,  4.01s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45/48 [01:10<00:12,  4.17s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 46/48 [01:11<00:06,  3.42s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 47/48 [01:16<00:03,  3.74s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:43<00:00, 10.58s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:43<00:00,  2.15s/it]
   âœ“ Evaluation complete
   ğŸ’¾ Saved evaluation metrics: naive_evaluation_metrics.parquet

ğŸ” Evaluating bm25...
Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]Evaluating:   2%|â–         | 1/48 [00:01<01:09,  1.47s/it]Evaluating:   4%|â–         | 2/48 [00:01<00:38,  1.20it/s]Evaluating:   6%|â–‹         | 3/48 [00:03<00:56,  1.25s/it]Evaluating:  10%|â–ˆ         | 5/48 [00:05<00:42,  1.02it/s]Evaluating:  12%|â–ˆâ–        | 6/48 [00:05<00:36,  1.15it/s]Evaluating:  15%|â–ˆâ–        | 7/48 [00:06<00:38,  1.07it/s]Evaluating:  17%|â–ˆâ–‹        | 8/48 [00:07<00:35,  1.14it/s]Evaluating:  21%|â–ˆâ–ˆ        | 10/48 [00:08<00:27,  1.37it/s]Evaluating:  23%|â–ˆâ–ˆâ–       | 11/48 [00:09<00:23,  1.59it/s]Evaluating:  25%|â–ˆâ–ˆâ–Œ       | 12/48 [00:09<00:17,  2.01it/s]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 14/48 [00:09<00:13,  2.60it/s]Evaluating:  31%|â–ˆâ–ˆâ–ˆâ–      | 15/48 [00:10<00:13,  2.40it/s]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–      | 16/48 [00:11<00:22,  1.42it/s]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 19/48 [00:13<00:17,  1.66it/s]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 20/48 [00:15<00:27,  1.01it/s]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/48 [00:16<00:22,  1.18it/s]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 22/48 [00:16<00:20,  1.28it/s]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 24/48 [00:17<00:14,  1.68it/s]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 25/48 [00:19<00:23,  1.02s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 27/48 [00:21<00:19,  1.10it/s]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 29/48 [00:23<00:18,  1.02it/s]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 30/48 [00:25<00:20,  1.14s/it]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/48 [00:26<00:18,  1.11s/it]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 32/48 [00:27<00:16,  1.05s/it]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 34/48 [00:29<00:15,  1.07s/it]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 35/48 [00:29<00:12,  1.03it/s]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 36/48 [00:32<00:16,  1.37s/it]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 37/48 [00:34<00:15,  1.44s/it]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 38/48 [00:34<00:12,  1.20s/it]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 39/48 [00:42<00:26,  2.95s/it]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 40/48 [00:46<00:26,  3.33s/it]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 41/48 [00:50<00:24,  3.52s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 42/48 [01:01<00:35,  5.85s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 43/48 [01:02<00:21,  4.23s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 44/48 [01:07<00:18,  4.52s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45/48 [01:13<00:15,  5.06s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 46/48 [01:17<00:09,  4.59s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:38<00:00,  7.39s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:38<00:00,  2.06s/it]
   âœ“ Evaluation complete
   ğŸ’¾ Saved evaluation metrics: bm25_evaluation_metrics.parquet

ğŸ” Evaluating ensemble...
Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]Evaluating:   2%|â–         | 1/48 [00:01<01:28,  1.87s/it]Evaluating:   4%|â–         | 2/48 [00:02<01:00,  1.32s/it]Evaluating:   8%|â–Š         | 4/48 [00:03<00:26,  1.65it/s]Evaluating:  10%|â–ˆ         | 5/48 [00:03<00:26,  1.64it/s]Evaluating:  12%|â–ˆâ–        | 6/48 [00:04<00:28,  1.45it/s]Evaluating:  15%|â–ˆâ–        | 7/48 [00:04<00:21,  1.90it/s]Evaluating:  17%|â–ˆâ–‹        | 8/48 [00:05<00:20,  1.94it/s]Evaluating:  19%|â–ˆâ–‰        | 9/48 [00:06<00:22,  1.70it/s]Evaluating:  21%|â–ˆâ–ˆ        | 10/48 [00:07<00:33,  1.15it/s]Evaluating:  23%|â–ˆâ–ˆâ–       | 11/48 [00:07<00:25,  1.48it/s]Evaluating:  25%|â–ˆâ–ˆâ–Œ       | 12/48 [00:09<00:36,  1.00s/it]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 13/48 [00:13<01:01,  1.74s/it]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 14/48 [00:14<00:54,  1.59s/it]Evaluating:  31%|â–ˆâ–ˆâ–ˆâ–      | 15/48 [00:15<00:46,  1.41s/it]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–      | 16/48 [00:16<00:39,  1.23s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 20/48 [00:18<00:21,  1.28it/s]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 22/48 [00:18<00:16,  1.58it/s]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 23/48 [00:19<00:17,  1.40it/s]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 24/48 [00:20<00:19,  1.24it/s]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 25/48 [00:22<00:20,  1.10it/s]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/48 [00:26<00:37,  1.72s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 27/48 [00:28<00:40,  1.92s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 30/48 [00:30<00:22,  1.24s/it]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/48 [00:31<00:17,  1.03s/it]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 32/48 [00:32<00:19,  1.21s/it]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 33/48 [00:33<00:14,  1.00it/s]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 34/48 [00:34<00:15,  1.13s/it]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 35/48 [00:38<00:22,  1.75s/it]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 36/48 [00:40<00:23,  1.97s/it]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 37/48 [00:45<00:28,  2.63s/it]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 38/48 [00:45<00:20,  2.09s/it]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 39/48 [00:54<00:36,  4.03s/it]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 40/48 [01:06<00:50,  6.26s/it]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 41/48 [01:19<00:59,  8.48s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 42/48 [01:20<00:36,  6.12s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 44/48 [01:21<00:14,  3.54s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45/48 [01:24<00:10,  3.36s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 46/48 [01:26<00:06,  3.04s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 47/48 [01:34<00:04,  4.53s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:51<00:00,  8.02s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:51<00:00,  2.33s/it]
   âœ“ Evaluation complete
   ğŸ’¾ Saved evaluation metrics: ensemble_evaluation_metrics.parquet

ğŸ” Evaluating cohere_rerank...
Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]Evaluating:   2%|â–         | 1/48 [00:02<02:06,  2.69s/it]Evaluating:   4%|â–         | 2/48 [00:03<01:15,  1.65s/it]Evaluating:   8%|â–Š         | 4/48 [00:04<00:33,  1.33it/s]Evaluating:  12%|â–ˆâ–        | 6/48 [00:04<00:19,  2.11it/s]Evaluating:  15%|â–ˆâ–        | 7/48 [00:05<00:26,  1.56it/s]Evaluating:  17%|â–ˆâ–‹        | 8/48 [00:06<00:24,  1.66it/s]Evaluating:  25%|â–ˆâ–ˆâ–Œ       | 12/48 [00:07<00:16,  2.22it/s]Evaluating:  31%|â–ˆâ–ˆâ–ˆâ–      | 15/48 [00:07<00:10,  3.11it/s]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–      | 16/48 [00:09<00:15,  2.13it/s]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 18/48 [00:09<00:12,  2.35it/s]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 19/48 [00:10<00:16,  1.71it/s]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 20/48 [00:11<00:18,  1.53it/s]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 22/48 [00:13<00:18,  1.38it/s]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 23/48 [00:14<00:19,  1.29it/s]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 25/48 [00:15<00:13,  1.67it/s]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/48 [00:16<00:16,  1.32it/s]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 27/48 [00:17<00:19,  1.10it/s]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 30/48 [00:19<00:13,  1.38it/s]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/48 [00:22<00:18,  1.11s/it]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 33/48 [00:23<00:15,  1.01s/it]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 34/48 [00:24<00:11,  1.20it/s]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 35/48 [00:26<00:16,  1.28s/it]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 36/48 [00:27<00:14,  1.23s/it]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 37/48 [00:29<00:14,  1.33s/it]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 38/48 [00:32<00:16,  1.68s/it]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 39/48 [00:42<00:35,  3.94s/it]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 40/48 [00:45<00:31,  3.88s/it]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 41/48 [00:51<00:30,  4.30s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 42/48 [01:09<00:50,  8.36s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 43/48 [01:10<00:31,  6.33s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 44/48 [01:15<00:23,  5.76s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45/48 [01:17<00:14,  4.83s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 46/48 [01:20<00:08,  4.33s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 47/48 [01:23<00:03,  3.90s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:29<00:00,  4.43s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:29<00:00,  1.87s/it]
   âœ“ Evaluation complete
   ğŸ’¾ Saved evaluation metrics: cohere_rerank_evaluation_metrics.parquet

âœ“ All evaluations complete!

================================================================================
STEP 5: COMPARATIVE ANALYSIS
================================================================================

ğŸ’¾ Saved comparative results: comparative_ragas_results.parquet

================================================================================
COMPARATIVE RAGAS RESULTS
================================================================================

    Retriever  Faithfulness  Answer Relevancy  Context Precision  Context Recall  Average
Cohere Rerank        0.9548            0.9456             0.9097          0.9673   0.9443
         Bm25        0.9346            0.9461             0.8605          0.9881   0.9323
        Naive        0.9219            0.9482             0.8468          0.9881   0.9262
     Ensemble        0.9169            0.9435             0.8277          1.0000   0.9220

ğŸ† Winner: Cohere Rerank with 94.43% average score
   Improvement over baseline: +2.0%

================================================================================
EVALUATION SUMMARY
================================================================================

End time: 2025-10-20 14:02:03

Test Set:
  - Dataset: dwb2023/gdelt-rag-golden-testset
  - Questions: 12

Retrievers Evaluated:
  naive, bm25, ensemble, cohere_rerank

Output Files (/home/donbr/don-aie-cohort8/cert-challenge/data/processed):
  - Evaluation inputs: 4 Ã— *_evaluation_inputs.parquet (6 columns: RAG outputs)
  - Evaluation metrics: 4 Ã— *_evaluation_metrics.parquet (10 columns: RAG + RAGAS)
  - Comparative summary: comparative_ragas_results.parquet
  - Provenance manifest: RUN_MANIFEST.json

Metrics Computed:
  - Faithfulness (answer grounded in context)
  - Answer Relevancy (answer addresses question)
  - Context Precision (relevant contexts ranked higher)
  - Context Recall (ground truth coverage)

All results saved to: /home/donbr/don-aie-cohort8/cert-challenge/data/processed


================================================================================
STEP 6: GENERATING RUN MANIFEST
================================================================================

ğŸ’¾ Saved run manifest: RUN_MANIFEST.json
   âœ“ RAGAS version: 0.2.10
   âœ“ Python version: 3.11
   âœ“ Retriever configs: 4
   âœ“ Evaluation settings captured

================================================================================
âœ… EVALUATION COMPLETE
================================================================================
