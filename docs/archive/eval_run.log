/home/donbr/don-aie-cohort8/cert-challenge/src/config.py:79: UserWarning: Api key is used with an insecure connection.
  return QdrantClient(url=QDRANT_URL, **kwargs)
================================================================================
RAGAS EVALUATION HARNESS (Using src/ Modules)
================================================================================

Start time: 2025-10-20 13:48:20
✓ OpenAI API key configured
✓ Cohere API key configured (cohere_rerank will work)
✓ Linked to ingestion manifest: ragas_pi...

================================================================================
STEP 1: LOADING DATA
================================================================================

Loading source documents from dwb2023/gdelt-rag-sources...
✓ Loaded 38 source documents

Loading golden testset from dwb2023/gdelt-rag-golden-testset...
✓ Loaded 12 test examples
  Columns: ['user_input', 'reference_contexts', 'reference', 'synthesizer_name']

================================================================================
STEP 2: BUILDING RAG STACK
================================================================================

Creating vector store (recreate=False)...
✓ Vector store connected (reusing existing collection)

Creating retrievers (k=5)...
✓ Created 4 retrievers: ['naive', 'bm25', 'ensemble', 'cohere_rerank']

Building LangGraph workflows...
✓ Built 4 compiled graphs

================================================================================
STEP 3: RUNNING INFERENCE
================================================================================

Processing 12 questions × 4 retrievers...

📊 Processing naive retriever...
   ✓ Processed 12 questions
   💾 Saved inference results: naive_evaluation_inputs.parquet

📊 Processing bm25 retriever...
   ✓ Processed 12 questions
   💾 Saved inference results: bm25_evaluation_inputs.parquet

📊 Processing ensemble retriever...
   ✓ Processed 12 questions
   💾 Saved inference results: ensemble_evaluation_inputs.parquet

📊 Processing cohere_rerank retriever...
   ✓ Processed 12 questions
   💾 Saved inference results: cohere_rerank_evaluation_inputs.parquet

✓ All inference complete! Results saved to /home/donbr/don-aie-cohort8/cert-challenge/data/processed

================================================================================
STEP 4: RAGAS EVALUATION
================================================================================

Metrics: Faithfulness, Answer Relevancy, Context Precision, Context Recall

🔍 Evaluating naive...
Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]Evaluating:   2%|▏         | 1/48 [00:02<01:34,  2.01s/it]Evaluating:   4%|▍         | 2/48 [00:03<01:12,  1.58s/it]Evaluating:  12%|█▎        | 6/48 [00:05<00:29,  1.41it/s]Evaluating:  19%|█▉        | 9/48 [00:06<00:24,  1.60it/s]Evaluating:  21%|██        | 10/48 [00:07<00:24,  1.56it/s]Evaluating:  23%|██▎       | 11/48 [00:07<00:22,  1.64it/s]Evaluating:  25%|██▌       | 12/48 [00:09<00:26,  1.34it/s]Evaluating:  27%|██▋       | 13/48 [00:09<00:23,  1.50it/s]Evaluating:  29%|██▉       | 14/48 [00:10<00:21,  1.61it/s]Evaluating:  33%|███▎      | 16/48 [00:10<00:18,  1.77it/s]Evaluating:  38%|███▊      | 18/48 [00:12<00:19,  1.51it/s]Evaluating:  40%|███▉      | 19/48 [00:12<00:16,  1.79it/s]Evaluating:  42%|████▏     | 20/48 [00:13<00:15,  1.84it/s]Evaluating:  44%|████▍     | 21/48 [00:14<00:21,  1.25it/s]Evaluating:  46%|████▌     | 22/48 [00:15<00:18,  1.44it/s]Evaluating:  48%|████▊     | 23/48 [00:18<00:35,  1.42s/it]Evaluating:  54%|█████▍    | 26/48 [00:18<00:15,  1.44it/s]Evaluating:  56%|█████▋    | 27/48 [00:20<00:17,  1.19it/s]Evaluating:  58%|█████▊    | 28/48 [00:21<00:20,  1.04s/it]Evaluating:  60%|██████    | 29/48 [00:22<00:16,  1.14it/s]Evaluating:  62%|██████▎   | 30/48 [00:26<00:29,  1.64s/it]Evaluating:  65%|██████▍   | 31/48 [00:26<00:21,  1.24s/it]Evaluating:  67%|██████▋   | 32/48 [00:27<00:20,  1.30s/it]Evaluating:  73%|███████▎  | 35/48 [00:29<00:10,  1.22it/s]Evaluating:  75%|███████▌  | 36/48 [00:30<00:11,  1.05it/s]Evaluating:  77%|███████▋  | 37/48 [00:34<00:18,  1.67s/it]Evaluating:  79%|███████▉  | 38/48 [00:36<00:16,  1.60s/it]Evaluating:  81%|████████▏ | 39/48 [00:37<00:13,  1.48s/it]Evaluating:  83%|████████▎ | 40/48 [00:47<00:30,  3.87s/it]Evaluating:  85%|████████▌ | 41/48 [00:49<00:23,  3.32s/it]Evaluating:  88%|████████▊ | 42/48 [01:01<00:34,  5.69s/it]Evaluating:  90%|████████▉ | 43/48 [01:03<00:23,  4.76s/it]Evaluating:  92%|█████████▏| 44/48 [01:05<00:16,  4.01s/it]Evaluating:  94%|█████████▍| 45/48 [01:10<00:12,  4.17s/it]Evaluating:  96%|█████████▌| 46/48 [01:11<00:06,  3.42s/it]Evaluating:  98%|█████████▊| 47/48 [01:16<00:03,  3.74s/it]Evaluating: 100%|██████████| 48/48 [01:43<00:00, 10.58s/it]Evaluating: 100%|██████████| 48/48 [01:43<00:00,  2.15s/it]
   ✓ Evaluation complete
   💾 Saved evaluation metrics: naive_evaluation_metrics.parquet

🔍 Evaluating bm25...
Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]Evaluating:   2%|▏         | 1/48 [00:01<01:09,  1.47s/it]Evaluating:   4%|▍         | 2/48 [00:01<00:38,  1.20it/s]Evaluating:   6%|▋         | 3/48 [00:03<00:56,  1.25s/it]Evaluating:  10%|█         | 5/48 [00:05<00:42,  1.02it/s]Evaluating:  12%|█▎        | 6/48 [00:05<00:36,  1.15it/s]Evaluating:  15%|█▍        | 7/48 [00:06<00:38,  1.07it/s]Evaluating:  17%|█▋        | 8/48 [00:07<00:35,  1.14it/s]Evaluating:  21%|██        | 10/48 [00:08<00:27,  1.37it/s]Evaluating:  23%|██▎       | 11/48 [00:09<00:23,  1.59it/s]Evaluating:  25%|██▌       | 12/48 [00:09<00:17,  2.01it/s]Evaluating:  29%|██▉       | 14/48 [00:09<00:13,  2.60it/s]Evaluating:  31%|███▏      | 15/48 [00:10<00:13,  2.40it/s]Evaluating:  33%|███▎      | 16/48 [00:11<00:22,  1.42it/s]Evaluating:  40%|███▉      | 19/48 [00:13<00:17,  1.66it/s]Evaluating:  42%|████▏     | 20/48 [00:15<00:27,  1.01it/s]Evaluating:  44%|████▍     | 21/48 [00:16<00:22,  1.18it/s]Evaluating:  46%|████▌     | 22/48 [00:16<00:20,  1.28it/s]Evaluating:  50%|█████     | 24/48 [00:17<00:14,  1.68it/s]Evaluating:  52%|█████▏    | 25/48 [00:19<00:23,  1.02s/it]Evaluating:  56%|█████▋    | 27/48 [00:21<00:19,  1.10it/s]Evaluating:  60%|██████    | 29/48 [00:23<00:18,  1.02it/s]Evaluating:  62%|██████▎   | 30/48 [00:25<00:20,  1.14s/it]Evaluating:  65%|██████▍   | 31/48 [00:26<00:18,  1.11s/it]Evaluating:  67%|██████▋   | 32/48 [00:27<00:16,  1.05s/it]Evaluating:  71%|███████   | 34/48 [00:29<00:15,  1.07s/it]Evaluating:  73%|███████▎  | 35/48 [00:29<00:12,  1.03it/s]Evaluating:  75%|███████▌  | 36/48 [00:32<00:16,  1.37s/it]Evaluating:  77%|███████▋  | 37/48 [00:34<00:15,  1.44s/it]Evaluating:  79%|███████▉  | 38/48 [00:34<00:12,  1.20s/it]Evaluating:  81%|████████▏ | 39/48 [00:42<00:26,  2.95s/it]Evaluating:  83%|████████▎ | 40/48 [00:46<00:26,  3.33s/it]Evaluating:  85%|████████▌ | 41/48 [00:50<00:24,  3.52s/it]Evaluating:  88%|████████▊ | 42/48 [01:01<00:35,  5.85s/it]Evaluating:  90%|████████▉ | 43/48 [01:02<00:21,  4.23s/it]Evaluating:  92%|█████████▏| 44/48 [01:07<00:18,  4.52s/it]Evaluating:  94%|█████████▍| 45/48 [01:13<00:15,  5.06s/it]Evaluating:  96%|█████████▌| 46/48 [01:17<00:09,  4.59s/it]Evaluating: 100%|██████████| 48/48 [01:38<00:00,  7.39s/it]Evaluating: 100%|██████████| 48/48 [01:38<00:00,  2.06s/it]
   ✓ Evaluation complete
   💾 Saved evaluation metrics: bm25_evaluation_metrics.parquet

🔍 Evaluating ensemble...
Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]Evaluating:   2%|▏         | 1/48 [00:01<01:28,  1.87s/it]Evaluating:   4%|▍         | 2/48 [00:02<01:00,  1.32s/it]Evaluating:   8%|▊         | 4/48 [00:03<00:26,  1.65it/s]Evaluating:  10%|█         | 5/48 [00:03<00:26,  1.64it/s]Evaluating:  12%|█▎        | 6/48 [00:04<00:28,  1.45it/s]Evaluating:  15%|█▍        | 7/48 [00:04<00:21,  1.90it/s]Evaluating:  17%|█▋        | 8/48 [00:05<00:20,  1.94it/s]Evaluating:  19%|█▉        | 9/48 [00:06<00:22,  1.70it/s]Evaluating:  21%|██        | 10/48 [00:07<00:33,  1.15it/s]Evaluating:  23%|██▎       | 11/48 [00:07<00:25,  1.48it/s]Evaluating:  25%|██▌       | 12/48 [00:09<00:36,  1.00s/it]Evaluating:  27%|██▋       | 13/48 [00:13<01:01,  1.74s/it]Evaluating:  29%|██▉       | 14/48 [00:14<00:54,  1.59s/it]Evaluating:  31%|███▏      | 15/48 [00:15<00:46,  1.41s/it]Evaluating:  33%|███▎      | 16/48 [00:16<00:39,  1.23s/it]Evaluating:  42%|████▏     | 20/48 [00:18<00:21,  1.28it/s]Evaluating:  46%|████▌     | 22/48 [00:18<00:16,  1.58it/s]Evaluating:  48%|████▊     | 23/48 [00:19<00:17,  1.40it/s]Evaluating:  50%|█████     | 24/48 [00:20<00:19,  1.24it/s]Evaluating:  52%|█████▏    | 25/48 [00:22<00:20,  1.10it/s]Evaluating:  54%|█████▍    | 26/48 [00:26<00:37,  1.72s/it]Evaluating:  56%|█████▋    | 27/48 [00:28<00:40,  1.92s/it]Evaluating:  62%|██████▎   | 30/48 [00:30<00:22,  1.24s/it]Evaluating:  65%|██████▍   | 31/48 [00:31<00:17,  1.03s/it]Evaluating:  67%|██████▋   | 32/48 [00:32<00:19,  1.21s/it]Evaluating:  69%|██████▉   | 33/48 [00:33<00:14,  1.00it/s]Evaluating:  71%|███████   | 34/48 [00:34<00:15,  1.13s/it]Evaluating:  73%|███████▎  | 35/48 [00:38<00:22,  1.75s/it]Evaluating:  75%|███████▌  | 36/48 [00:40<00:23,  1.97s/it]Evaluating:  77%|███████▋  | 37/48 [00:45<00:28,  2.63s/it]Evaluating:  79%|███████▉  | 38/48 [00:45<00:20,  2.09s/it]Evaluating:  81%|████████▏ | 39/48 [00:54<00:36,  4.03s/it]Evaluating:  83%|████████▎ | 40/48 [01:06<00:50,  6.26s/it]Evaluating:  85%|████████▌ | 41/48 [01:19<00:59,  8.48s/it]Evaluating:  88%|████████▊ | 42/48 [01:20<00:36,  6.12s/it]Evaluating:  92%|█████████▏| 44/48 [01:21<00:14,  3.54s/it]Evaluating:  94%|█████████▍| 45/48 [01:24<00:10,  3.36s/it]Evaluating:  96%|█████████▌| 46/48 [01:26<00:06,  3.04s/it]Evaluating:  98%|█████████▊| 47/48 [01:34<00:04,  4.53s/it]Evaluating: 100%|██████████| 48/48 [01:51<00:00,  8.02s/it]Evaluating: 100%|██████████| 48/48 [01:51<00:00,  2.33s/it]
   ✓ Evaluation complete
   💾 Saved evaluation metrics: ensemble_evaluation_metrics.parquet

🔍 Evaluating cohere_rerank...
Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]Evaluating:   2%|▏         | 1/48 [00:02<02:06,  2.69s/it]Evaluating:   4%|▍         | 2/48 [00:03<01:15,  1.65s/it]Evaluating:   8%|▊         | 4/48 [00:04<00:33,  1.33it/s]Evaluating:  12%|█▎        | 6/48 [00:04<00:19,  2.11it/s]Evaluating:  15%|█▍        | 7/48 [00:05<00:26,  1.56it/s]Evaluating:  17%|█▋        | 8/48 [00:06<00:24,  1.66it/s]Evaluating:  25%|██▌       | 12/48 [00:07<00:16,  2.22it/s]Evaluating:  31%|███▏      | 15/48 [00:07<00:10,  3.11it/s]Evaluating:  33%|███▎      | 16/48 [00:09<00:15,  2.13it/s]Evaluating:  38%|███▊      | 18/48 [00:09<00:12,  2.35it/s]Evaluating:  40%|███▉      | 19/48 [00:10<00:16,  1.71it/s]Evaluating:  42%|████▏     | 20/48 [00:11<00:18,  1.53it/s]Evaluating:  46%|████▌     | 22/48 [00:13<00:18,  1.38it/s]Evaluating:  48%|████▊     | 23/48 [00:14<00:19,  1.29it/s]Evaluating:  52%|█████▏    | 25/48 [00:15<00:13,  1.67it/s]Evaluating:  54%|█████▍    | 26/48 [00:16<00:16,  1.32it/s]Evaluating:  56%|█████▋    | 27/48 [00:17<00:19,  1.10it/s]Evaluating:  62%|██████▎   | 30/48 [00:19<00:13,  1.38it/s]Evaluating:  65%|██████▍   | 31/48 [00:22<00:18,  1.11s/it]Evaluating:  69%|██████▉   | 33/48 [00:23<00:15,  1.01s/it]Evaluating:  71%|███████   | 34/48 [00:24<00:11,  1.20it/s]Evaluating:  73%|███████▎  | 35/48 [00:26<00:16,  1.28s/it]Evaluating:  75%|███████▌  | 36/48 [00:27<00:14,  1.23s/it]Evaluating:  77%|███████▋  | 37/48 [00:29<00:14,  1.33s/it]Evaluating:  79%|███████▉  | 38/48 [00:32<00:16,  1.68s/it]Evaluating:  81%|████████▏ | 39/48 [00:42<00:35,  3.94s/it]Evaluating:  83%|████████▎ | 40/48 [00:45<00:31,  3.88s/it]Evaluating:  85%|████████▌ | 41/48 [00:51<00:30,  4.30s/it]Evaluating:  88%|████████▊ | 42/48 [01:09<00:50,  8.36s/it]Evaluating:  90%|████████▉ | 43/48 [01:10<00:31,  6.33s/it]Evaluating:  92%|█████████▏| 44/48 [01:15<00:23,  5.76s/it]Evaluating:  94%|█████████▍| 45/48 [01:17<00:14,  4.83s/it]Evaluating:  96%|█████████▌| 46/48 [01:20<00:08,  4.33s/it]Evaluating:  98%|█████████▊| 47/48 [01:23<00:03,  3.90s/it]Evaluating: 100%|██████████| 48/48 [01:29<00:00,  4.43s/it]Evaluating: 100%|██████████| 48/48 [01:29<00:00,  1.87s/it]
   ✓ Evaluation complete
   💾 Saved evaluation metrics: cohere_rerank_evaluation_metrics.parquet

✓ All evaluations complete!

================================================================================
STEP 5: COMPARATIVE ANALYSIS
================================================================================

💾 Saved comparative results: comparative_ragas_results.parquet

================================================================================
COMPARATIVE RAGAS RESULTS
================================================================================

    Retriever  Faithfulness  Answer Relevancy  Context Precision  Context Recall  Average
Cohere Rerank        0.9548            0.9456             0.9097          0.9673   0.9443
         Bm25        0.9346            0.9461             0.8605          0.9881   0.9323
        Naive        0.9219            0.9482             0.8468          0.9881   0.9262
     Ensemble        0.9169            0.9435             0.8277          1.0000   0.9220

🏆 Winner: Cohere Rerank with 94.43% average score
   Improvement over baseline: +2.0%

================================================================================
EVALUATION SUMMARY
================================================================================

End time: 2025-10-20 14:02:03

Test Set:
  - Dataset: dwb2023/gdelt-rag-golden-testset
  - Questions: 12

Retrievers Evaluated:
  naive, bm25, ensemble, cohere_rerank

Output Files (/home/donbr/don-aie-cohort8/cert-challenge/data/processed):
  - Evaluation inputs: 4 × *_evaluation_inputs.parquet (6 columns: RAG outputs)
  - Evaluation metrics: 4 × *_evaluation_metrics.parquet (10 columns: RAG + RAGAS)
  - Comparative summary: comparative_ragas_results.parquet
  - Provenance manifest: RUN_MANIFEST.json

Metrics Computed:
  - Faithfulness (answer grounded in context)
  - Answer Relevancy (answer addresses question)
  - Context Precision (relevant contexts ranked higher)
  - Context Recall (ground truth coverage)

All results saved to: /home/donbr/don-aie-cohort8/cert-challenge/data/processed


================================================================================
STEP 6: GENERATING RUN MANIFEST
================================================================================

💾 Saved run manifest: RUN_MANIFEST.json
   ✓ RAGAS version: 0.2.10
   ✓ Python version: 3.11
   ✓ Retriever configs: 4
   ✓ Evaluation settings captured

================================================================================
✅ EVALUATION COMPLETE
================================================================================
